{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ujdhO_AMlaxh"
      },
      "source": [
        "<div align=\"center\" dir=\"auto\">\n",
        "<p dir=\"auto\"><a href=\"https://colab.research.google.com/github/encord-team/encord-notebooks/blob/main/colab-notebooks/Encord_Notebooks_How_To_Fine_Tuning_SAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/1a088dbe61584d1b213517c05e313c765ca127ce21118626b2d0db0b8f573235/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f656e636f72642d7465616d2f656e636f72642d616374697665\"><img src=\"https://camo.githubusercontent.com/1a088dbe61584d1b213517c05e313c765ca127ce21118626b2d0db0b8f573235/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f656e636f72642d7465616d2f656e636f72642d616374697665\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/encord-team/encord-active\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://www.piwheels.org/project/encord-active/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ad0f13bbb94beadb953a46e51cee0db17ade62e4deb8c306e36a7addfa69d18f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f656e636f72642d616374697665\" alt=\"PyPi project\" data-canonical-src=\"https://img.shields.io/pypi/v/encord-active\" style=\"max-width: 100%;\"></a>\n",
        "<a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/890ceffab6602e1a6ecac875e179b3984708f98b0a91293e6c632752d11bad7e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f656e636f72642d616374697665\"><img src=\"https://camo.githubusercontent.com/890ceffab6602e1a6ecac875e179b3984708f98b0a91293e6c632752d11bad7e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f656e636f72642d616374697665\" alt=\"PyPi version\" data-canonical-src=\"https://img.shields.io/pypi/pyversions/encord-active\" style=\"max-width: 100%;\"></a>\n",
        "\n",
        "<a href=\"https://docs.encord.com/active/docs\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/457447b616a4e5aecccd5351fa78d41ce828901e395b935ad2a94d94db75e5d0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6f6e6c696e652d626c7565\" alt=\"docs\" data-canonical-src=\"https://img.shields.io/badge/docs-online-blue\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://join.slack.com/t/encordactive/shared_invite/zt-1hc2vqur9-Fzj1EEAHoqu91sZ0CX0A7Q\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "<img alt=\"Join us on Slack\" src=\"https://img.shields.io/badge/Join_Our_Community-4A154B?label=&logo=slack&logoColor=white\">\n",
        "</a>\n",
        "<a href=\"https://github.com/encord-team/encord-notebooks/blob/main/local-notebooks/Encord_Notebooks_How_To_Fine_Tuning_SAM.ipynb\" rel=\"nofollow\"><img src=\"https://badges.aleen42.com/src/github.svg\" alt=\"&quot;Encord Notebooks;\" data-canonical-src=\"https://badges.aleen42.com/src/github.svg\" style=\"max-width: 100%;\"></a>\n",
        "\n",
        "\n",
        "</p>\n",
        "<p dir=\"auto\">\n",
        "<a href=\"https://twitter.com/encord_team\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c7ea37e4d1f0c2e900069152f5990d3bcef4a69c97dede26f08d588958410bd8/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f656e636f72645f7465616d3f6c6162656c3d253430656e636f72645f7465616d267374796c653d736f6369616c\" alt=\"Twitter Follow\" data-canonical-src=\"https://img.shields.io/twitter/follow/encord_team?label=%40encord_team&amp;style=social\" style=\"max-width: 100%;\"></a></p>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lOXCsrzileKN"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <p>\n",
        "    <a align=\"center\" href=\"\" target=\"_blank\">\n",
        "      <img\n",
        "        width=\"7232\"\n",
        "        src=\"https://storage.googleapis.com/encord-notebooks/encord_active_notebook_banner.png\">\n",
        "    </a>\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tbJThT5Ol_lr"
      },
      "source": [
        "# üü£ Encord Notebooks | üîß How to fine-tune Segment Anything Model (SAM)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n6eLcQDjmvmg"
      },
      "source": [
        "## üèÅ Overview"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aiehQvBj5Crc"
      },
      "source": [
        "üëã Hi there!\n",
        "\n",
        "\n",
        "\n",
        "This is the notebook gives you a walkthrough on fine-tuning [Segment Anything Model](https://encord.com/blog/segment-anything-model-explained/) (SAM) to a specific application.\n",
        "\n",
        "You will use the stamp verification dataset on [Kaggle]( https://www.kaggle.com/datasets/rtatman/stamp-verification-staver-dataset) since it has:\n",
        "* data SAM is unlikely to have seen (scans of invoices with stamps),\n",
        "* precise ground truth segmentation masks,\n",
        "* and bounding boxes which we can use as prompts to SAM.\n",
        "\n",
        "This tutorial has been prepared by [Alex Bonnet](https://encord.com/author/alexandre-bonnet/), ML Solutions Engineer at Encord.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "> üí° If you want to read more about Encord Active checkout our [GitHub](https://github.com/encord-team/encord-active) and [documentation](https://docs.encord.com/docs/active-overview).\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YSsqv3dWoVQ6"
      },
      "source": [
        " ## üì∞ Complementary Blog Post"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-qRmUVKwDu-9"
      },
      "source": [
        "![How To Fine-Tune Segment Anything - Encord Blog](https://images.prismic.io/encord/fc9dadaa-a011-4de1-b0eb-e7a55f854081_Group%2048096157.png?ixlib=gatsbyFP&auto=compress%2Cformat&fit=max)\n",
        "\n",
        "This notebook implements the steps discussed in the blog post: https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/\n",
        "\n",
        "Check it üîº out for a comprehensive walkthrough."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8xFZSDikKMsA"
      },
      "source": [
        "## üì• Installation and Set Up"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n7rshBkIH3bl"
      },
      "source": [
        "To ensure a smooth experience with this walkthrough notebook, you need to install the necessary libraries, dependencies, and model family. This step is essential for running the code and executing the examples effectively.\n",
        "\n",
        "By installing these libraries upfront, you'll have everything you need to follow along and explore the notebook without any interruptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0oru8hAn6q2"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle &> /dev/null\n",
        "! pip install torch torchvision &> /dev/null\n",
        "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
        "! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null\n",
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwevXNd2Ofw"
      },
      "source": [
        "**Action Required:** Place your kaggle.json file into the files in the notebook workspace. More info here https://github.com/Kaggle/kaggle-api#api-credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uTL0fDZEOnl"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! mv kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD5Kt6lO_HIw"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download rtatman/stamp-verification-staver-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJP-eL2_EA52"
      },
      "outputs": [],
      "source": [
        "! unzip stamp-verification-staver-dataset.zip &> /dev/null"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dXzO_ZRWIEmz"
      },
      "source": [
        "## üì© Importing Relevant Libraries"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I52JuXm7IVWt"
      },
      "source": [
        "In this section, you will import the key libraries that will be used for dataset manipulation and visualization. These libraries play a crucial role in executing the code examples and demonstrating the concepts covered in the walkthrough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwmQm0C3n_3D"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv4ob2wRE9CS"
      },
      "outputs": [],
      "source": [
        "# Exclude scans with zero or multiple bboxes (of the first 100)\n",
        "stamps_to_exclude = {\n",
        "    'stampDS-00008',\n",
        "    'stampDS-00010',\n",
        "    'stampDS-00015',\n",
        "    'stampDS-00021',\n",
        "    'stampDS-00027',\n",
        "    'stampDS-00031',\n",
        "    'stampDS-00039',\n",
        "    'stampDS-00041',\n",
        "    'stampDS-00049',\n",
        "    'stampDS-00053',\n",
        "    'stampDS-00059',\n",
        "    'stampDS-00069',\n",
        "    'stampDS-00073',\n",
        "    'stampDS-00080',\n",
        "    'stampDS-00090',\n",
        "    'stampDS-00098',\n",
        "    'stampDS-00100'\n",
        "}.union({\n",
        "    'stampDS-00012',\n",
        "    'stampDS-00013',\n",
        "    'stampDS-00014',\n",
        "}) # Exclude 3 scans that aren't the type of scan we want to be fine tuning for"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WNJGn1BsKUMS"
      },
      "source": [
        "## üõ†Ô∏è Preprocess the dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KhXGI8HXyIFi"
      },
      "source": [
        "You'll need to preprocess the scans from numpy arrays to pytorch tensors. To do this, follow what happens inside [`SamPredictor.set_image`](https://github.com/facebookresearch/segment-anything/blob/c1910835a32a05cbb79bdacbec8f25914a7e3a20/segment_anything/predictor.py#L34-L60) and [`SamPredictor.set_torch_image`](https://github.com/facebookresearch/segment-anything/blob/c1910835a32a05cbb79bdacbec8f25914a7e3a20/segment_anything/predictor.py#L63) which preprocesses the image.\n",
        "\n",
        "\n",
        "\n",
        "First, extract the bounding box coordinates which will be used to feed into SAM as prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNrV8CN8F9G0"
      },
      "outputs": [],
      "source": [
        "bbox_coords = {}\n",
        "for f in sorted(Path('ground-truth-maps/ground-truth-maps/').iterdir())[:100]:\n",
        "  k = f.stem[:-3]\n",
        "  if k not in stamps_to_exclude:\n",
        "    im = cv2.imread(f.as_posix())\n",
        "    gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
        "    contours, hierarchy = cv2.findContours(gray,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
        "    if len(contours) > 1:\n",
        "      x,y,w,h = cv2.boundingRect(contours[0])\n",
        "      height, width, _ = im.shape\n",
        "      bbox_coords[k] = np.array([x, y, x + w, y + h])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zsv0IGBDyMkS"
      },
      "source": [
        "Extract the ground truth segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz7B4NDoJRxJ"
      },
      "outputs": [],
      "source": [
        "ground_truth_masks = {}\n",
        "for k in bbox_coords.keys():\n",
        "  gt_grayscale = cv2.imread(f'ground-truth-pixel/ground-truth-pixel/{k}-px.png', cv2.IMREAD_GRAYSCALE)\n",
        "  ground_truth_masks[k] = (gt_grayscale == 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FsX7SxD8KYOP"
      },
      "source": [
        "## üëÄ Inspect the images, bounding box prompts, and the ground truth segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz8C8QaxoT6N"
      },
      "outputs": [],
      "source": [
        "# Helper functions provided in https://github.com/facebookresearch/segment-anything/blob/9e8f1309c94f1128a6e5c047a10fdcb02fc8d651/notebooks/predictor_example.ipynb\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ncVrlh5fyed9"
      },
      "source": [
        "We can see here that the ground truth mask is extremely tight which will be good for calculating an accurate loss.\n",
        "The bounding box overlaid will be a good prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3csOAxFju_Pi"
      },
      "outputs": [],
      "source": [
        "name = 'stampDS-00004'\n",
        "image = cv2.imread(f'scans/scans/{name}.png')\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "show_box(bbox_coords[name], plt.gca())\n",
        "show_mask(ground_truth_masks[name], plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IJIFDGaUKfQp"
      },
      "source": [
        "## üßë‚Äçüç≥ Prepare Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdTD9CTxKena"
      },
      "outputs": [],
      "source": [
        "model_type = 'vit_b'\n",
        "checkpoint = 'sam_vit_b_01ec64.pth'\n",
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjTIJtLxP8ZG"
      },
      "outputs": [],
      "source": [
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "sam_model.to(device)\n",
        "sam_model.train();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MKZFlHjdKlhr"
      },
      "source": [
        "### üîÅ Convert the input images into a format SAM's internal functions expect."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu0MdejGylZY"
      },
      "source": [
        "First, use [`utils.transform.ResizeLongestSide`](https://github.com/facebookresearch/segment-anything/blob/c1910835a32a05cbb79bdacbec8f25914a7e3a20/segment_anything/predictor.py#L31) to resize the image, as this is the transformer used inside the predictor.\n",
        "\n",
        "Then convert the image to a pytorch tensor and use the SAM's [preprocess method](https://github.com/facebookresearch/segment-anything/blob/c1910835a32a05cbb79bdacbec8f25914a7e3a20/segment_anything/modeling/sam.py#L164) to finish preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtPYpirbK3Wi"
      },
      "outputs": [],
      "source": [
        "# Preprocess the images\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "transformed_data = defaultdict(dict)\n",
        "for k in bbox_coords.keys():\n",
        "  image = cv2.imread(f'scans/scans/{k}.png')\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "  input_image = transform.apply_image(image)\n",
        "  input_image_torch = torch.as_tensor(input_image, device=device)\n",
        "  transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
        "\n",
        "  input_image = sam_model.preprocess(transformed_image)\n",
        "  original_image_size = image.shape[:2]\n",
        "  input_size = tuple(transformed_image.shape[-2:])\n",
        "\n",
        "  transformed_data[k]['image'] = input_image\n",
        "  transformed_data[k]['input_size'] = input_size\n",
        "  transformed_data[k]['original_image_size'] = original_image_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxnY6TMGKjdc"
      },
      "outputs": [],
      "source": [
        "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
        "lr = 1e-4\n",
        "wd = 0\n",
        "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "# loss_fn = torch.nn.BCELoss()\n",
        "keys = list(bbox_coords.keys())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sRHCNdzZy3dt"
      },
      "source": [
        "## üöÄ Run SAM Fine-Tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9DIYcFKu14nr"
      },
      "source": [
        "This is the main training loop.\n",
        "\n",
        "Improvements to be made include batching and moving the computation of the image and prompt embeddings outside the loop since we are not tuning these parts of the model, this will speed up training as we should not recompute the embeddings during each epoch.\n",
        "\n",
        "> ‚ö†Ô∏è Sometimes the optimizer gets lost in the parameter space and the loss function blows up. Restarting from scratch (including running all cells below 'Prepare Fine Tuning' in order to start with default weights again) should solve it.\n",
        "\n",
        "üìù In a production implementation, a better choice of optimiser/loss function will certainly help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRQ6yd_PM_B9"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.nn.functional import threshold, normalize\n",
        "\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_losses = []\n",
        "  # Just train on the first 20 examples\n",
        "  for k in keys[:20]:\n",
        "    input_image = transformed_data[k]['image'].to(device)\n",
        "    input_size = transformed_data[k]['input_size']\n",
        "    original_image_size = transformed_data[k]['original_image_size']\n",
        "\n",
        "    # No grad here as we don't want to optimise the encoders\n",
        "    with torch.no_grad():\n",
        "      image_embedding = sam_model.image_encoder(input_image)\n",
        "\n",
        "      prompt_box = bbox_coords[k]\n",
        "      box = transform.apply_boxes(prompt_box, original_image_size)\n",
        "      box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
        "      box_torch = box_torch[None, :]\n",
        "\n",
        "      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
        "          points=None,\n",
        "          boxes=box_torch,\n",
        "          masks=None,\n",
        "      )\n",
        "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
        "      image_embeddings=image_embedding,\n",
        "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
        "      sparse_prompt_embeddings=sparse_embeddings,\n",
        "      dense_prompt_embeddings=dense_embeddings,\n",
        "      multimask_output=False,\n",
        "    )\n",
        "\n",
        "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
        "    binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
        "\n",
        "    gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masks[k], (1, 1, ground_truth_masks[k].shape[0], ground_truth_masks[k].shape[1]))).to(device)\n",
        "    gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
        "\n",
        "    loss = loss_fn(binary_mask, gt_binary_mask)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_losses.append(loss.item())\n",
        "  losses.append(epoch_losses)\n",
        "  print(f'EPOCH: {epoch}')\n",
        "  print(f'Mean loss: {mean(epoch_losses)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKqIxUgAOTzp"
      },
      "outputs": [],
      "source": [
        "mean_losses = [mean(x) for x in losses]\n",
        "mean_losses\n",
        "\n",
        "plt.plot(list(range(len(mean_losses))), mean_losses)\n",
        "plt.title('Mean epoch loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TuDlIiRjmitT"
      },
      "source": [
        "## üìè Compare the fine-tuned model to the original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9fZiPoIKXYW"
      },
      "outputs": [],
      "source": [
        "# Load up the model with default weights\n",
        "sam_model_orig = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "sam_model_orig.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dIKKKHOn_7R"
      },
      "outputs": [],
      "source": [
        "# Set up predictors for both tuned and original models\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "predictor_tuned = SamPredictor(sam_model)\n",
        "predictor_original = SamPredictor(sam_model_orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhNHx-6kpEWu"
      },
      "outputs": [],
      "source": [
        "# The model has not seen keys[21] (or keys[20]) since we only trained on keys[:20]\n",
        "k = keys[21]\n",
        "image = cv2.imread(f'scans/scans/{k}.png')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "predictor_tuned.set_image(image)\n",
        "predictor_original.set_image(image)\n",
        "\n",
        "input_bbox = np.array(bbox_coords[k])\n",
        "\n",
        "masks_tuned, _, _ = predictor_tuned.predict(\n",
        "    point_coords=None,\n",
        "    box=input_bbox,\n",
        "    multimask_output=False,\n",
        ")\n",
        "\n",
        "masks_orig, _, _ = predictor_original.predict(\n",
        "    point_coords=None,\n",
        "    box=input_bbox,\n",
        "    multimask_output=False,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Df2oxBaxxXrt"
      },
      "source": [
        "See here that the tuned model is starting to ignore the whitespace between the words, which is what the ground truths show. With further training, more data and further hyperparameter tuning you will be able to improve this result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH6NorejpTii"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "_, axs = plt.subplots(1, 2, figsize=(25, 25))\n",
        "\n",
        "\n",
        "axs[0].imshow(image)\n",
        "show_mask(masks_tuned, axs[0])\n",
        "show_box(input_bbox, axs[0])\n",
        "axs[0].set_title('Mask with Tuned Model', fontsize=26)\n",
        "axs[0].axis('off')\n",
        "\n",
        "\n",
        "axs[1].imshow(image)\n",
        "show_mask(masks_orig, axs[1])\n",
        "show_box(input_bbox, axs[1])\n",
        "axs[1].set_title('Mask with Untuned Model', fontsize=26)\n",
        "axs[1].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H8eMyK1vNP4J"
      },
      "source": [
        "# ‚úÖ Wrap up"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ysWfdTinMjeI"
      },
      "source": [
        "If the image does not render due to size limitations, you can view it here:\n",
        "\n",
        "![fine-tuned model vs sam model - encord notebooks](https://storage.googleapis.com/encord-notebooks/fine-tune%20SAM/tuned_model_comparison.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-w9Hph-7NLnm"
      },
      "source": [
        "\n",
        "üììThis Colab notebook showed you how to fine-tune Segment Anything Model (SAM) on your own data. If you would like to learn more, check out the [complementary blog post](https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/).\n",
        "\n",
        "---\n",
        "\n",
        "üü£ Encord Active is an open-source framework for computer vision model testing, evaluation, and validation. **Check out the project on [GitHub](https://github.com/encord-team/encord-active), leave a star üåü** if you like it. We welcome you to [contribute](https://docs.encord.com/docs/active-contributing) if you find something is missing.\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Check out our üìñ[blog](https://encord.com/blog/) and üì∫[YouTube](https://www.youtube.com/@encord) channel to stay up-to-date with the latest in computer vision, foundation models, active learning, and data-centric AI.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpkoq9YIQGWI"
      },
      "source": [
        "### ‚ú® Want more walthroughs like this? Check out the üü£ [Encord Notebooks repository](https://github.com/encord-team/encord-notebooks/tree/9617d8bc6cea52563ecb18bf173c2043195403e8)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
