{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ylvIgFjIDkqZ"
      },
      "source": [
        "<div align=\"center\" dir=\"auto\">\n",
        "<p dir=\"auto\"><a href=\"https://colab.research.google.com/github/encord-team/encord-notebooks/blob/main/colab-notebooks/Encord_Notebooks_Team_gDINO_SAM_vs_maskrcnn_webinar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<div align=\"center\" dir=\"auto\">\n",
        "  <div style=\"flex: 1; padding: 10px;\">\n",
        "    <a href=\"https://join.slack.com/t/encordactive/shared_invite/zt-1hc2vqur9-Fzj1EEAHoqu91sZ0CX0A7Q\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Join us on Slack\" src=\"https://img.shields.io/badge/Join_Our_Community-4A154B?label=&logo=slack&logoColor=white\">\n",
        "    </a>\n",
        "    <a href=\"https://docs.encord.com/docs/active-overview\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Documentation\" src=\"https://img.shields.io/badge/docs-Online-blue\">\n",
        "    </a>\n",
        "    <a href=\"https://twitter.com/encord_team\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/encord_team?label=%40encord_team&amp;style=social\">\n",
        "    </a>\n",
        "    <img alt=\"Python versions\" src=\"https://img.shields.io/pypi/pyversions/encord-active\">\n",
        "    <a href=\"https://pypi.org/project/encord-active/\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"PyPi project\" src=\"https://img.shields.io/pypi/v/encord-active\">\n",
        "    </a>\n",
        "    <a href=\"https://docs.encord.com/docs/active-contributing\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"PRs Welcome\" src=\"https://img.shields.io/badge/PRs-Welcome-blue\">\n",
        "    </a>\n",
        "    <img alt=\"License\" src=\"https://img.shields.io/github/license/encord-team/encord-active\">\n",
        "  </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GtuzJGZLDw7K"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <p>\n",
        "    <a align=\"center\" href=\"\" target=\"_blank\">\n",
        "      <img\n",
        "        width=\"7232\"\n",
        "        src=\"https://storage.googleapis.com/encord-notebooks/encord_active_notebook_banner.png\">\n",
        "    </a>\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QPl6cV9UElI6"
      },
      "source": [
        "# üü£ Encord Notebooks | üÜö Grounding-DINO+SAM vs. Mask-RCNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5PWbl8dwFNKq"
      },
      "source": [
        "## üèÅ Overview"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqHaLcV2oq_D"
      },
      "source": [
        "üëã Hi there!\n",
        "\n",
        "In this notebook file, you will get the segmentation predictions of images using Grounding-DINO and Segment Anything Model (SAM).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "üí°If you want to read more about üü£ Encord Active checkout our [GitHub](https://github.com/encord-team/encord-active) and [documentation](https://encord-active-docs.web.app/).\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o9MBKP_XGBkb"
      },
      "source": [
        " ## üìΩÔ∏è Complementary Webinar"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OueBGAyOGZmq"
      },
      "source": [
        "[![Are VFMs on par with SOTA - GroundingDIN+SAM vs MaskRCNN](https://storage.googleapis.com/encord-notebooks/ground_dino_sam/Encord_Webinar_Are_VFMs_on_par_with_SOTA.jpeg)](https://encord.com/learning-hub/are-vfms-on-par-with-sota/)\n",
        "\n",
        "With Foundational Models increasing in prominence, Encord's President and Co-Founder sits down with Lead ML Engineer, Frederik to dissect Meta's new Visual Foundation Model, Segment Anything Model (SAM).\n",
        "\n",
        "After combining the model with Grounding-DINO to allow for zero-shot segmentation, the team will compare it to a SOTA Mask-RCNN model to see whether the development of SAM really is revolutionary for segmentation. They discussed:\n",
        "\n",
        "- The rise of VFMs and how they differ from standard models\n",
        "- What Meta's release of DINOv2 means for Grounding-DINO + SAM\n",
        "- How SAM and Grounding-DINO compare to previous segmentation models for performance and predictions\n",
        "- Evaluating model performance using Encord Active\n",
        "\n",
        "Check out [the webinar](https://encord.com/learning-hub/are-vfms-on-par-with-sota/)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UZFcgyHspbWz"
      },
      "source": [
        "## üì• Installation and Set Up: Grounding-DINO and Segment Anything Model (SAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpa3d-3boPFJ"
      },
      "outputs": [],
      "source": [
        "%cd /content #ENTER YOUR WORKING DIRECTORY\n",
        "%git clone https://github.com/IDEA-Research/Grounded-Segment-Anything\n",
        "%cd /content/Grounded-Segment-Anything  # CHANGE TO YOUR WORKING DIRECTORY\n",
        "%pip install -q -r requirements.txt\n",
        "\n",
        "%cd /content/Grounded-Segment-Anything/GroundingDINO # CHANGE TO YOUR WORKING DIRECTORY\n",
        "%pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything/segment_anything # CHANGE TO YOUR WORKING DIRECTORY\n",
        "%pip install -q ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ha9tNEsI5Bb"
      },
      "source": [
        "# üì® Import all the necessary libraries"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gXFZzGTmI7Az"
      },
      "source": [
        "In this section, you will import the key libraries that will be used for running the code sample. These libraries play a crucial role in executing the code examples and demonstrating the concepts covered in the walkthrough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zQFnxZ65eUW"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# ‚ö†Ô∏è REMEMBER TO CHANGE TO THE CORRECT DIRECTORY üëá\n",
        "\n",
        "module_paths = [\n",
        "    \"/content/Grounded-Segment-Anything/GroundingDINO\",\n",
        "    \"/content/Grounded-Segment-Anything/segment_anything\",\n",
        "]\n",
        "for path in module_paths:\n",
        "    if not path in sys.path:\n",
        "        sys.path.append(path)\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "import groundingdino.datasets.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util import box_ops\n",
        "from groundingdino.util.inference import load_image, predict\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
        "from huggingface_hub import hf_hub_download\n",
        "from matplotlib.patches import Rectangle\n",
        "from PIL import Image\n",
        "from segment_anything import SamAutomaticMaskGenerator, SamPredictor, sam_model_registry\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Qmbn4mJItS"
      },
      "source": [
        "# ‚öì Try GroundingDINO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "La8k0q5B4dgb"
      },
      "source": [
        "\n",
        "Let's start by trying out GroundingDINO.\n",
        "What we will have to do is\n",
        "\n",
        "1. Fetch a test image\n",
        "2. Define what we're searchnig for (the prompt)\n",
        "3. Download the model weights and load the model\n",
        "4. Run model inference on our example image and search query\n",
        "5. Displaying the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "gpRmy3304VG8",
        "outputId": "9aa361b5-a762-4de4-c6eb-a730e0306c02"
      },
      "outputs": [],
      "source": [
        "#@title ### üíª Fetch an image for testing\n",
        "# Here, we will use one of the demo images from the GroundedSAM repo\n",
        "dino_image_transform =  transform = T.Compose(\n",
        "    [\n",
        "        T.RandomResize([800], max_size=1333),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def load_image(image_path):\n",
        "    # load image\n",
        "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "    image, _ = dino_image_transform(image_pil, None)\n",
        "    return image_pil, image\n",
        "\n",
        "\n",
        "# ‚ö†Ô∏è REMEMBER TO CHANGE TO THE CORRECT DIRECTORY üëá\n",
        "\n",
        "image_path = \"/content/Grounded-Segment-Anything/assets/demo6.jpg\"\n",
        "img, img_tensor = load_image(image_path)\n",
        "\n",
        "plt.imshow(img)\n",
        "_ = plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L5QTYuUU51rb"
      },
      "outputs": [],
      "source": [
        "#@title ### üí¨ Define query string\n",
        "# Grounding dino suggests to encode classes by joining them with dots.\n",
        "\n",
        "class_descriptions = \".\".join([\"cat\", \"dog\", \"horse\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOOxZYta4P1d",
        "outputId": "cecf4086-5c09-4d08-9b14-8e07b8bad1f6"
      },
      "outputs": [],
      "source": [
        "#@title üì• Download model weights for GroundingDINO and load model\n",
        "def load_dino_from_hf(device, repo_id=\"ShilongLiu/GroundingDINO\", filename=\"groundingdino_swinb_cogcoor.pth\", ckpt_config_filename=\"GroundingDINO_SwinB.cfg.py\"):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    args.device = device\n",
        "    model = build_model(args)\n",
        "    checkpoint = torch.load(cache_file, map_location=\"cpu\")\n",
        "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
        "    _ = model.eval()\n",
        "    return model\n",
        "\n",
        "dino_model = load_dino_from_hf(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4fuq0JEGNzG",
        "outputId": "8bc9a983-c96e-4842-be10-9b74fc4b64a8"
      },
      "outputs": [],
      "source": [
        "#@title ### ü™Ñ Run GroundingDINO\n",
        "def get_grounding_output(dino_model, image, class_description, device, box_threshold=0.3, text_threshold=0.25):\n",
        "    class_description = class_description.lower()\n",
        "    class_description = class_description.strip()\n",
        "    if not class_description.endswith(\".\"):\n",
        "        class_description = class_description + \".\"\n",
        "\n",
        "    dino_model = dino_model.to(device)\n",
        "    image = image.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = dino_model(image[None], captions=[class_description])\n",
        "    logits = outputs[\"pred_logits\"].cpu().sigmoid()[0]  # (nq, 256)\n",
        "    boxes = outputs[\"pred_boxes\"].cpu()[0]  # (nq, 4)\n",
        "    logits.shape[0]\n",
        "\n",
        "    # filter output\n",
        "    logits_filt = logits.clone()\n",
        "    boxes_filt = boxes.clone()\n",
        "    filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
        "    logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
        "    boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
        "    logits_filt.shape[0]\n",
        "\n",
        "    # get phrase\n",
        "    tokenlizer = dino_model.tokenizer\n",
        "    tokenized = tokenlizer(class_description)\n",
        "\n",
        "    # build pred descriptions\n",
        "    pred_phrases = []\n",
        "    for logit, box in zip(logits_filt, boxes_filt):\n",
        "        pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)\n",
        "        pred_phrases.append(pred_phrase + f\" ({str(logit.max().item())[:4]})\")\n",
        "\n",
        "    return boxes_filt, pred_phrases\n",
        "\n",
        "boxes, phrases = get_grounding_output(dino_model, img_tensor, class_descriptions, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Gqcrq-P3JmBQ",
        "outputId": "b92b0e84-7aab-402e-fc59-3f6735c3fe54"
      },
      "outputs": [],
      "source": [
        "#@title ### üñºÔ∏è Display results\n",
        "def plot_boxes(image_pil, boxes, labels):\n",
        "    W, H = image_pil.size\n",
        "    assert len(boxes) == len(labels), \"boxes and labels must have same length\"\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(image_pil)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    for box, label in zip(boxes, labels):\n",
        "        box = box * torch.Tensor([W, H, W, H])\n",
        "        # from cxcyywh to xywh\n",
        "        box[:2] -= box[2:] / 2\n",
        "        x, y, w, h = box\n",
        "\n",
        "        color = tuple(np.random.random(size=3).tolist())\n",
        "\n",
        "        ax.add_patch(Rectangle((x, y), w, h, color=color, fill=None))\n",
        "        ax.add_patch(Rectangle((x, y), w, h, color=color, fill=color, alpha=0.3))\n",
        "        ax.text(x, y, str(label), color=\"white\", ha=\"left\", va=\"top\")\n",
        "\n",
        "    return ax\n",
        "\n",
        "image_with_box = plot_boxes(img, boxes, phrases)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VySBRoCpJVEB"
      },
      "source": [
        "# ü™£ Try SAM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dEV0exVaNE4n"
      },
      "source": [
        "\n",
        "Let's also try out SAM. We will follow similar steps as above - but with the SAM weights and code:\n",
        "\n",
        "1. ~Load image~ üëà we already did this\n",
        "2. Define what we're searching for (the prompt)\n",
        "3. Download and load the model\n",
        "4. Running model inference\n",
        "5. Displaying results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "mKRD4w81NECY",
        "outputId": "a72ea432-14cf-4d44-bca1-3343715a493b"
      },
      "outputs": [],
      "source": [
        "#@title ### üí¨ Define what you're searching for\n",
        "#@markdown We will try out a couple of bounding boxes although SAM offers more than that.\n",
        "#@markdown Let's have a look at the image again - with axis.\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(img)\n",
        "\n",
        "#@markdown it seems like some good points would be:\n",
        "boxes = np.array([\n",
        "    (50, 0, 1000, 900),\n",
        "    (1550, 550, 1800, 1150),\n",
        "])\n",
        "\n",
        "def show_box(box, ax, color=\"red\"):\n",
        "    x1, y1, x2, y2 = box\n",
        "    w, h = x2 - x1, y2 - y1\n",
        "    ax.add_patch(Rectangle((x1, y1), w, h, color=color, fill=None))\n",
        "\n",
        "for box in boxes:\n",
        "    show_box(box, ax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7hthJY7UWq2",
        "outputId": "0de55d9e-67cf-4d1c-b731-12151555f0f5"
      },
      "outputs": [],
      "source": [
        "#@title ### üì• Download and load SAM\n",
        "%cd /content\n",
        "def load_sam_model(device,  model_file: Path = Path(\"sam_vit_h_4b8939.pth\"), model_type = \"default\"):\n",
        "# def load_sam_model(device,  model_file: Path = Path(\"sam_vit_b_01ec64.pth\"), model_type = \"default\"):\n",
        "    print(model_file)\n",
        "    if not model_file.exists():\n",
        "        # Hack for UTF-8 input encoding\n",
        "        import locale\n",
        "        def getpreferredencoding(do_setlocale = True):\n",
        "            return \"UTF-8\"\n",
        "        locale.getpreferredencoding = getpreferredencoding\n",
        "        # Hack end\n",
        "        # !wget https://dl.fbaipublicfiles.com/segment_anything/\n",
        "        model_name = model_file.name\n",
        "        import subprocess\n",
        "        subprocess.run(f\"wget https://dl.fbaipublicfiles.com/segment_anything/{model_name}\", shell=True)\n",
        "\n",
        "    sam = sam_model_registry[model_type](checkpoint=model_file.name)\n",
        "    sam.to(device=device)\n",
        "    sam_model = SamPredictor(sam)\n",
        "    return sam_model\n",
        "\n",
        "sam_model = load_sam_model(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BjUZl1OzYiPn"
      },
      "outputs": [],
      "source": [
        "#@title ### ü™Ñ Run inference\n",
        "def get_sam_output(sam_model, img, boxes, device):\n",
        "    img_np = np.asarray(img)\n",
        "    sam_model.set_image(img_np)\n",
        "\n",
        "    boxes_tensor = torch.tensor(boxes, device=device)\n",
        "    boxes_sam = sam_model.transform.apply_boxes_torch(boxes_tensor, img_np.shape[:2])\n",
        "\n",
        "    masks, *_ = sam_model.predict_torch(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        boxes=boxes_sam,\n",
        "        multimask_output=False,\n",
        "    )\n",
        "    return masks.detach().cpu().numpy().squeeze()\n",
        "\n",
        "masks = get_sam_output(sam_model, img, boxes, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "djP9sGqCZ0Kh",
        "outputId": "190bd625-6793-4cc4-b763-46e85152fa52"
      },
      "outputs": [],
      "source": [
        "#@title ### üñºÔ∏è Display SAM results\n",
        "def show_mask(mask, ax):\n",
        "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "    return color\n",
        "\n",
        "def plot_masks(img, masks):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    for mask in masks:\n",
        "        show_mask(mask, ax)\n",
        "    return fig\n",
        "\n",
        "print(masks.shape)\n",
        "_ = plot_masks(img, masks)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7446a5FEJ3uO"
      },
      "source": [
        "# ü™¢ Combine GroundingDINO and Segment Anything Model (SAM)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qvqn83O1p4-d"
      },
      "source": [
        "Combining the two models is straight forward. We take the boxes from GroundingDINO and feed them to SAM and voila, you have your predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "gB2-e5x_p4Ar",
        "outputId": "c0b16c8e-4548-448c-b706-0d84eec455e7"
      },
      "outputs": [],
      "source": [
        "def transform_boxes_to_xyxy(boxes, img_w, img_h):\n",
        "    boxes = boxes * torch.tensor([img_w, img_h, img_w, img_h], device=boxes.device)\n",
        "    boxes[:,:2] -= boxes[:,2:] / 2\n",
        "    boxes[:,2:] = boxes[:,:2] + boxes[:,2:]\n",
        "    return boxes.numpy()\n",
        "\n",
        "def plot_masks_with_labels(img, masks, boxes, labels):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    for mask, box, label in zip(masks, boxes, labels):\n",
        "        color = show_mask(mask, ax)\n",
        "        show_box(box, ax, color=color)\n",
        "        x, y, *_ = box\n",
        "        ax.text(x, y, str(label), color=\"white\", ha=\"left\", va=\"top\")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def predict(img_path, class_descriptions):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    pil_img, tensor_img = load_image(img_path)\n",
        "    boxes, phrases = get_grounding_output(dino_model, tensor_img, class_descriptions, device)\n",
        "\n",
        "    if not boxes.size:\n",
        "        print(\"Couldn't find any boxes\")\n",
        "        return\n",
        "\n",
        "    boxes = transform_boxes_to_xyxy(boxes, pil_img.size[0], pil_img.size[1])\n",
        "    masks = get_sam_output(sam_model, pil_img, boxes, device)\n",
        "\n",
        "    if masks.sum():\n",
        "        if masks.ndim == 2:\n",
        "            masks = masks[None]\n",
        "        plot_masks_with_labels(pil_img, masks, boxes, phrases)\n",
        "    else:\n",
        "        print(\"No masks detected\")\n",
        "        plot_boxes(pil_img, boxes, phrases)\n",
        "\n",
        "predict(image_path, \"cat.dog.horse\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkus0VzfJ9_U"
      },
      "source": [
        "# ‚úÖ Wrap up"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F4xUej4kzcko"
      },
      "source": [
        "That's it folks!\n",
        "If you aren't done, please find our [notebook](./Encord_Notebooks_Zero_shot_image_segmentation_with_grounding_dino_and_sam.ipynb) the does the end-2-end experiments include model evaluation on an actual segmentation task."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rgTgJReJz2XY"
      },
      "source": [
        "# üñºÔ∏è Other examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c0h-Wv2j3H_k",
        "outputId": "51afc832-5232-480c-a637-857ce9384c3b"
      },
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è REMEMBER TO CHANGE TO THE CORRECT DIRECTORY üëá\n",
        "\n",
        "files = [\n",
        "    (\"/content/Grounded-Segment-Anything/assets/demo1.jpg\", \"bear.dog.chair.person.\"),\n",
        "    (\"/content/Grounded-Segment-Anything/assets/demo2.jpg\", \"bear.dog.chair.person.\"),\n",
        "    (\"/content/Grounded-Segment-Anything/assets/demo3.jpg\", \"bear.dog.chair.person.\"),\n",
        "    (\"/content/Grounded-Segment-Anything/assets/demo4.jpg\", \"bear.dog.chair.person.\"),\n",
        "    (\"/content/Grounded-Segment-Anything/assets/demo5.jpg\", \"bear.dog.chair.person.\"),\n",
        "]\n",
        "for f, q in files:\n",
        "    predict(f, q)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A-W7N6ohKGLP"
      },
      "source": [
        "üììThis Colab notebook showed you how to build combine the power of GroundingDINO and SAM to produce accurate segmentation masks and bounding boxes that compare with state-of-the-art techniques like [MaskRCNN](https://paperswithcode.com/paper/mask-r-cnn).\n",
        "\n",
        "---\n",
        "\n",
        "üü£ Encord Active is an open-source framework for computer vision model testing, evaluation, and validation. Check out the project on [GitHub](https://github.com/encord-team/encord-active), leave a star üåü if you like it, and leave an issue if you find something is missing.\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Check out our üìñ[blog](https://encord.com/blog/webinar-semantic-visual-search-chatgpt-clip/) and üì∫[YouTube](https://www.youtube.com/@encord) channel to stay up-to-date with the latest in computer vision, foundation models, active learning, and data-centric AI."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TnEPPjowLIqZ"
      },
      "source": [
        "### ‚ú® Want more walthroughs like this? Check out the üü£ [Encord Notebooks repository](https://github.com/encord-team/encord-notebooks/)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o9MBKP_XGBkb"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
