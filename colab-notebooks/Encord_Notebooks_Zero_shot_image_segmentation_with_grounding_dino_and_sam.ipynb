{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v9jvEq7F6FIR"
      },
      "source": [
        "<div align=\"center\" dir=\"auto\">\n",
        "<p dir=\"auto\"><a href=\"https://colab.research.google.com/github/encord-team/encord-notebooks/blob/main/colab-notebooks/Encord_Notebooks_Zero_shot_image_segmentation_with_grounding_dino_and_sam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<div align=\"center\" dir=\"auto\">\n",
        "  <div style=\"flex: 1; padding: 10px;\">\n",
        "    <a href=\"https://join.slack.com/t/encordactive/shared_invite/zt-1hc2vqur9-Fzj1EEAHoqu91sZ0CX0A7Q\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Join us on Slack\" src=\"https://img.shields.io/badge/Join_Our_Community-4A154B?label=&logo=slack&logoColor=white\">\n",
        "    </a>\n",
        "    <a href=\"https://docs.encord.com/docs/active-overview\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Documentation\" src=\"https://img.shields.io/badge/docs-Online-blue\">\n",
        "    </a>\n",
        "    <a href=\"https://twitter.com/encord_team\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/encord_team?label=%40encord_team&amp;style=social\">\n",
        "    </a>\n",
        "    <img alt=\"Python versions\" src=\"https://img.shields.io/pypi/pyversions/encord-active\">\n",
        "    <a href=\"https://pypi.org/project/encord-active/\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"PyPi project\" src=\"https://img.shields.io/pypi/v/encord-active\">\n",
        "    </a>\n",
        "    <a href=\"https://docs.encord.com/docs/active-contributing\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"PRs Welcome\" src=\"https://img.shields.io/badge/PRs-Welcome-blue\">\n",
        "    </a>\n",
        "    <img alt=\"Licence\" src=\"https://img.shields.io/github/license/encord-team/encord-active\">\n",
        "  </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SNhRaUNt6GqS"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <p>\n",
        "    <a align=\"center\" href=\"\" target=\"_blank\">\n",
        "      <img\n",
        "        width=\"7232\"\n",
        "        src=\"https://storage.googleapis.com/encord-notebooks/encord_active_notebook_banner.png\">\n",
        "    </a>\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9wlbB5Ia7PNM"
      },
      "source": [
        "# üü£ Encord Notebooks | üîß Zero-Shot Image Segmentation with Grounding-DINO + Segment Anything Model (SAM)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l1ZnlLnw7e4j"
      },
      "source": [
        "## üèÅ Overview"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqHaLcV2oq_D"
      },
      "source": [
        "üëã Hi there!\n",
        "\n",
        "In this notebook file, you will get and evaluate the segmentation predictions of images using [Grounding-DINO](https://encord.com/blog/grounding-dino-sam-vs-mask-rcnn-comparison/) and Segment Anything Model (SAM).\n",
        "\n",
        "You will use an üü£ Encord Active sandbox project to run the segmentation pipeline and visualize the prediction performance (mAP/mAR) on üü£ Encord Active as well.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "üí°If you want to read more about üü£ Encord Active checkout our [GitHub](https://github.com/encord-team/encord-active) and [documentation](https://encord-active-docs.web.app/)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "efUEK6Q_8CH1"
      },
      "source": [
        " ## üì∞ Complementary Blog Post"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kFn9avXB8DZV"
      },
      "source": [
        "![Encord_Notebooks_Grounding-DINO_Segment_Anything_Model_Header_Image](https://images.prismic.io/encord/63abbb6a-9fe0-4bd0-a76f-5cb4e0b1955a_Grounding-DINO%20%2B%20Segment%20Anything%20model%20Header%20image.png?ixlib=gatsbyFP&auto=compress%2Cformat&fit=max)\n",
        "\n",
        "This is the notebook which implements the steps discussed in this blog post: https://encord.com/blog/grounding-dino-sam-vs-mask-rcnn-comparison/\n",
        "\n",
        "Check it out for a complementary guide to this notebook."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UZFcgyHspbWz"
      },
      "source": [
        "## üì• Installation and Set Up: Grounding-DINO and Segment Anything Model (SAM)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BQQqf9dL-ciR"
      },
      "source": [
        "To ensure a smooth experience with this walkthrough notebook, you need to install the necessary libraries, dependencies, and model family. This step is essential for running the code and executing the examples effectively.\n",
        "\n",
        "By installing these libraries upfront, you'll have everything you need to follow along and explore the notebook without any interruptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpa3d-3boPFJ"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/IDEA-Research/Grounded-Segment-Anything\n",
        "\n",
        "%cd /content/Grounded-Segment-Anything\n",
        "!pip install -q -r requirements.txt\n",
        "%cd /content/Grounded-Segment-Anything/GroundingDINO\n",
        "!pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything/segment_anything\n",
        "!pip install -q ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E9EwvyFapgEA"
      },
      "source": [
        "## Install üü£ Encord Active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a9Z8p2RfCGx"
      },
      "outputs": [],
      "source": [
        "# Assert that python is 3.9 or 3.10 instead\n",
        "import sys\n",
        "assert sys.version_info.minor in [9, 10], \"Encord Active only supported for python 3.9 and 3.10.\"\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "!python -m pip install -qq encord-active\n",
        "\n",
        "display(Markdown('## ‚Äº Please restart your runtime before running the next cell.'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3jpJudrARqzp"
      },
      "source": [
        "## üì© Download an Encord Active sandbox project\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xPj52-vLtx4y"
      },
      "source": [
        "You will use the üü£ Encord Active quickstart project (200-images subset of COCO Val set) in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dij5XobYpvpy"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!encord-active download --project-name quickstart\n",
        "%cd /content/Grounded-Segment-Anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiNTlOcLvjSH"
      },
      "outputs": [],
      "source": [
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9W5CqDh9xqC8"
      },
      "outputs": [],
      "source": [
        "#@title üëáüèΩ Run this utility code\n",
        "import sys\n",
        "sys.stdout.fileno = lambda: 1\n",
        "sys.stderr.fileno = lambda: 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rfpqjsmDvu5t"
      },
      "source": [
        "## Load Grounding DINO and SAM models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jLJqSfCbvyaJ"
      },
      "source": [
        "\n",
        "You'll need to set up the necessary libraries, load the Grounding DINO model and the SAM model, and prepare the required data structures and objects for further processing.\n",
        "\n",
        "You need to load the Grounding DINO model using the function `load_model_hf` which takes repository ID, filenames, and device type as inputs. This function will download the model files from the Hugging Face model hub, build the model using the provided configuration, and load the model's state dictionary. It then sets the model to evaluation mode and returns the loaded model.\n",
        "\n",
        "The code will also read the ontology from a file and extracts the names of the objects and their corresponding feature node hashes. You'll set a text prompt using the names of the ontology objects and also the threshold values for box and text predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6LBbvKIqcIo"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Grounding DINO\n",
        "from GroundingDINO.groundingdino.models import build_model\n",
        "from GroundingDINO.groundingdino.util import box_ops\n",
        "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
        "from GroundingDINO.groundingdino.util.utils import clean_state_dict\n",
        "from GroundingDINO.groundingdino.util.inference import  load_image, predict\n",
        "\n",
        "\n",
        "# segment anything\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import cv2\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from tqdm import tqdm\n",
        "from encord_active.lib.project.project_file_structure import ProjectFileStructure\n",
        "from encord_active.lib.common.iterator import DatasetIterator\n",
        "from encord_active.lib.db.predictions import Format, ObjectDetection, Prediction\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Function to load Grounding DINO model from Hugging Face Hub\n",
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
        "    # Download and load model configuration\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "\n",
        "    # Build Grounding DINO model\n",
        "    model = build_model(args)\n",
        "    args.device = device\n",
        "\n",
        "    # Download and load model checkpoint\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
        "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    _ = model.eval()\n",
        "    return model\n",
        "\n",
        "device = \"cuda\"\n",
        "ea_project_path = Path('/content/quickstart') # Path to the project directory\n",
        "\n",
        "ckpt_repo_id = \"ShilongLiu/GroundingDINO\" # Repository ID of Grounding DINO model\n",
        "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\" # Model checkpoint filename\n",
        "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\" # Model configuration filename\n",
        "\n",
        "# Load Grounding DINO model\n",
        "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device=device)\n",
        "\n",
        "# Create ProjectFileStructure object for the project directory\n",
        "project_fs: ProjectFileStructure = ProjectFileStructure(ea_project_path)\n",
        "\n",
        "# Initialize DatasetIterator object with the project directory\n",
        "iterator = DatasetIterator(project_fs.project_dir)\n",
        "\n",
        "# Read ontology from file and extract object names and feature node hashes\n",
        "ontology = json.loads(project_fs.ontology.read_text(encoding=\"utf-8\"))\n",
        "ontology_names = [obj[\"name\"] for obj in ontology.get(\"objects\")]\n",
        "ontology_name_to_featurehash = {obj[\"name\"]: obj['featureNodeHash'] for obj in ontology.get(\"objects\")}\n",
        "\n",
        "TEXT_PROMPT = \" . \".join(ontology_names) # Set text prompt using ontology names\n",
        "BOX_TRESHOLD = 0.3 # Threshold for box predictions\n",
        "TEXT_TRESHOLD = 0.25 # Threshold for text predictions\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iBZpZ3MIwk4T"
      },
      "source": [
        "Initialize a SAM model using the specified checkpoint and model type you downloaded earlier, and move it to the the CUDA device.\n",
        "\n",
        "Next, you'll create a `SamPredictor` object with the initialized SAM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVGxSZ4-wkAi"
      },
      "outputs": [],
      "source": [
        "sam_checkpoint = 'sam_vit_b_01ec64.pth'\n",
        "model_type = \"vit_b\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint) # Initialize SAM model\n",
        "sam.to(device=device) # Move SAM model to cuda device\n",
        "sam_predictor = SamPredictor(sam) # Create SamPredictor object with SAM model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CBd1ijQVSDEd"
      },
      "source": [
        "## Import predictions to Encord Active"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FlZtvdGTxZKs"
      },
      "source": [
        "Now, you'll import the model predictions to üü£ Encord Active so you can see how the model's performance based on metric, evaluate its quality, identify failure modes, detect labelling errors and other valuable insights, enhancing the overall performance of the system.\n",
        "\n",
        "Essentially, you will use the Grounding-DINO model and the SAM model to perform object detection and segmentation, respectively, and incorporate the predictions to üü£Encord Active.\n",
        "\n",
        "> üí° Learn more in [the documentation](https://encord-active-docs.web.app/import/import-predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY4Hl8dvx9hm"
      },
      "outputs": [],
      "source": [
        "predictions_to_store = [] # List to store predictions\n",
        "\n",
        "# Iterate over the dataset using the DatasetIterator\n",
        "for data_unit, img_path in tqdm(iterator.iterate()):\n",
        "    try:\n",
        "        image_source, image = load_image(img_path.as_posix()) # Load the image\n",
        "\n",
        "        # Get bounding boxes from Grounding-DINO\n",
        "        boxes, logits, phrases = predict(\n",
        "            model=groundingdino_model,\n",
        "            image=image,\n",
        "            caption=TEXT_PROMPT,\n",
        "            box_threshold=BOX_TRESHOLD,\n",
        "            text_threshold=TEXT_TRESHOLD\n",
        "        )\n",
        "\n",
        "        if boxes.shape[0] > 0:\n",
        "\n",
        "            sam_predictor.set_image(image_source)\n",
        "\n",
        "            H, W, _ = image_source.shape\n",
        "            boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H]) # Convert box coordinates\n",
        "\n",
        "            transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_xyxy, image_source.shape[:2]).to(device)\n",
        "\n",
        "            # Get masks for bounding boxes using SAM predictor\n",
        "            masks, _, _ = sam_predictor.predict_torch(\n",
        "                point_coords=None,\n",
        "                point_labels=None,\n",
        "                boxes=transformed_boxes,\n",
        "                multimask_output=False,\n",
        "            )\n",
        "\n",
        "            for id, mask in enumerate(masks):\n",
        "                mask = mask[0].detach().cpu().numpy()  # Convert the mask to a numpy array\n",
        "                contours, hierarchy = cv2.findContours(mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) # Find contours in the mask\n",
        "\n",
        "                for contour in contours:\n",
        "                    contour = contour.reshape(contour.shape[0], 2) / np.array([[W, H]])\n",
        "\n",
        "                    if  phrases[id] not in ontology_name_to_featurehash:\n",
        "                        if phrases[id].split(\" \")[0] in ontology_name_to_featurehash:\n",
        "                            class_name = phrases[id].split(\" \")[0]\n",
        "                        else:\n",
        "                            class_name = ' '.join(phrases[id].split(' ')[:2]) # Extract class name from the phrase\n",
        "                    else:\n",
        "                        class_name = phrases[id]\n",
        "\n",
        "                    # Create a Prediction object with the predicted object detection information\n",
        "                    prediction = Prediction(\n",
        "                        data_hash=data_unit[\"data_hash\"],\n",
        "                        confidence=logits[id].item(),\n",
        "                        object=ObjectDetection(\n",
        "                            format=Format.POLYGON,\n",
        "                            data=contour,\n",
        "                            feature_hash=ontology_name_to_featurehash[class_name],\n",
        "                        ),\n",
        "                    )\n",
        "                    predictions_to_store.append(prediction) # Add the prediction to the list\n",
        "\n",
        "    except Exception as e:\n",
        "        print('Error')\n",
        "        print(e)\n",
        "\n",
        "# Save the predictions to a pickle file\n",
        "with open(os.path.join(project_fs.project_dir.as_posix(), f\"predictions_sam.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(predictions_to_store, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vxmUfdy3234F"
      },
      "source": [
        "## üì• Importing the `predictions_sam.pkl` file to Encord Active project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gePmdTFDJ9yC"
      },
      "source": [
        "\n",
        "\n",
        "- Download the predictions_sam.pkl file\n",
        "- Run `encord-active import predictions /path/to/predictions_sam.pkl -t /path/to/target/project/folder`\n",
        "- When the importing process is finished, you can open üü£ Encord Active to see the model quality results.\n",
        "\n",
        "Here are some screenshots from the model performance page of üü£ Encord Active:\n",
        "\n",
        "**Metric correlation**\n",
        "\n",
        "![Encord Notebooks - Metric Correlation Viz](https://storage.googleapis.com/encord-notebooks/ground_dino_sam/encord_notebooks_metric_correlation.png)\n",
        "\n",
        "\n",
        "**Metrics per class**\n",
        "\n",
        "![Encord Notebooks - Metrics per class](https://storage.googleapis.com/encord-notebooks/ground_dino_sam/encord_notebooks_metrics_per_class.png)\n",
        "\n",
        "**Performance by Metric**\n",
        "\n",
        "\n",
        "![Encord Notebooks - Performance by Metric](https://storage.googleapis.com/encord-notebooks/ground_dino_sam/encord_notebooks_performance_by_metric.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CgaPqAU32u0k"
      },
      "source": [
        "# ‚úÖ Wrap up"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GIZKh80123fc"
      },
      "source": [
        "\n",
        "üììThis Colab notebook showed you how to run zero-shot image segmentation with [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [Segment Anything Model](https://encord.com/blog/segment-anything-model-explained/) (SAM).\n",
        "\n",
        "Most importantly, you learnt how to import the model's predictions to Encord Active to analyse class errors and visualize the model's performance.\n",
        "\n",
        "If you would like to learn more, check out the [complementary blog post](https://encord.com/blog/grounding-dino-sam-vs-mask-rcnn-comparison/).\n",
        "\n",
        "---\n",
        "\n",
        "üü£ Encord Active is an open-source framework for computer vision model testing, evaluation, and validation. Check out the project on [GitHub](https://github.com/encord-team/encord-active), leave a star üåü if you like it, and leave an issue if you find something is missing.\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Check out our üìñ[blog](https://encord.com/blog/webinar-semantic-visual-search-chatgpt-clip/) and üì∫[YouTube](https://www.youtube.com/@encord) channel to stay up-to-date with the latest in computer vision, foundation models, active learning, and data-centric AI.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0an9L23G3t1c"
      },
      "source": [
        "### ‚ú® Want more walthroughs like this? Check out the üü£ [Encord Notebooks repository](https://github.com/encord-team/encord-notebooks/tree/9617d8bc6cea52563ecb18bf173c2043195403e8)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
