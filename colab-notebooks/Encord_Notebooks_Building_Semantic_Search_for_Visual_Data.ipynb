{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Pvokzj2Ae2"
      },
      "source": [
        "<div align=\"center\" dir=\"auto\">\n",
        "<p dir=\"auto\"><a href=\"https://colab.research.google.com/github/encord-team/encord-notebooks/blob/main/colab-notebooks/Encord_Notebooks_Building_Semantic_Search_for_Visual_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<div align=\"center\" dir=\"auto\">\n",
        "  <div style=\"flex: 1; padding: 10px;\">\n",
        "    <a href=\"https://join.slack.com/t/encordactive/shared_invite/zt-1hc2vqur9-Fzj1EEAHoqu91sZ0CX0A7Q\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Join us on Slack\" src=\"https://img.shields.io/badge/Join_Our_Community-4A154B?label=&logo=slack&logoColor=white\">\n",
        "    </a>\n",
        "    <a href=\"https://docs.encord.com/docs/active-overview\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Documentation\" src=\"https://img.shields.io/badge/docs-Online-blue\">\n",
        "    </a>\n",
        "    <a href=\"https://twitter.com/encord_team\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/encord_team?label=%40encord_team&amp;style=social\">\n",
        "    </a>\n",
        "    <img alt=\"Python versions\" src=\"https://img.shields.io/pypi/pyversions/encord-active\">\n",
        "    <a href=\"https://pypi.org/project/encord-active/\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"PyPi project\" src=\"https://img.shields.io/pypi/v/encord-active\">\n",
        "    </a>\n",
        "    <a href=\"https://docs.encord.com/docs/active-contributing\" target=\"_blank\" style=\"text-decoration:none\">\n",
        "      <img alt=\"PRs Welcome\" src=\"https://img.shields.io/badge/PRs-Welcome-blue\">\n",
        "    </a>\n",
        "    <img alt=\"Licence\" src=\"https://img.shields.io/github/license/encord-team/encord-active\">\n",
        "  </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G1A8rqIvHmkZ"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <p>\n",
        "    <a align=\"center\" href=\"\" target=\"_blank\">\n",
        "      <img\n",
        "        width=\"7232\"\n",
        "        src=\"https://storage.googleapis.com/encord-notebooks/encord_active_notebook_banner.png\">\n",
        "    </a>\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKTDxun2UnL"
      },
      "source": [
        "# ğŸŸ£ Encord Notebooks | ğŸ” Building Semantic Search for Visual Data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MavN92f-JSxY"
      },
      "source": [
        "## ğŸ Overview"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEXBlLjJQoY"
      },
      "source": [
        "ğŸ‘‹ Hi there! In this notebook, we will build a semantic search engine using CLIP and ChatGPT.\n",
        "\n",
        "We will use an ğŸŸ£ Encord-Active sandbox project to the search over.\n",
        "The dataset is to COCO Validation dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aHpx0kNd299C"
      },
      "source": [
        "## ğŸ“¥ Install ğŸŸ£ Encord-Active\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4B_e9Gt5JVIZ"
      },
      "source": [
        "ğŸ‘Ÿ Run the following script to install ğŸŸ£[Encord Active](https://docs.encord.com/active/docs/).\n",
        "\n",
        "<br>\n",
        "\n",
        "ğŸ“Œ  `python3.9` and `python3.10` are the version requirements to run ğŸŸ£Encord Active.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "ğŸ‘‰ Depending on your internet speed this might take 1-3 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOVM55XJ3Dio"
      },
      "outputs": [],
      "source": [
        "# Assert that python is 3.9 or 3.10 instead\n",
        "import sys\n",
        "assert sys.version_info.minor in [9, 10], \"Encord Active only supported for python 3.9 and 3.10.\"\n",
        "\n",
        "# Install Encord Active\n",
        "!python -m pip install -qq encord-active==0.1.60"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V8yUUayLPv2O"
      },
      "source": [
        "> # Please _RESTART_ your runtime before going any further.\n",
        "We've noticed some complications with the latest version of Google Colab and Numpy, which is fixed by restarting the runtime."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J4EjZixRQLo9"
      },
      "source": [
        "Later, we'll also need the `openai` and `langchain` modules, so let's install them as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLOXJMrxSeby"
      },
      "outputs": [],
      "source": [
        "!python -m pip install -qq langchain openai"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fZS7ktx74Vz3"
      },
      "source": [
        "## ğŸ“© Download an ğŸŸ£ Encord Active sandbox project\n",
        "\n",
        "ğŸŒ† We will use the [COCO Validation set](https://paperswithcode.com/dataset/coco) project for this notebook ğŸ“™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy9PZ4IR4mkE"
      },
      "outputs": [],
      "source": [
        "project_name = \"[open-source][validation]-coco-2017-dataset\"\n",
        "!encord-active download --project-name $project_name"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rAqm2jbK5Cat"
      },
      "source": [
        "# ğŸ“¨ Import all the necessary libraries"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H4c46b0gJdxh"
      },
      "source": [
        "In this section, you will import the key libraries that will be used for building the semantic search engine. These libraries play a crucial role in executing the code examples and demonstrating the concepts covered in the walkthrough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdpLp1215H3b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from functools import reduce\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from time import perf_counter\n",
        "from typing import List\n",
        "\n",
        "import clip\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import umap\n",
        "from encord_active.lib.common.image_utils import show_image_and_draw_polygons\n",
        "from encord_active.lib.common.iterator import DatasetIterator\n",
        "from encord_active.lib.db.connection import DBConnection\n",
        "from encord_active.lib.db.merged_metrics import (\n",
        "    MergedMetrics,\n",
        "    ensure_initialised_merged_metrics,\n",
        ")\n",
        "from encord_active.lib.project.project import Project\n",
        "from faiss import IndexFlatIP\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser, RetryWithErrorOutputParser\n",
        "from langchain.prompts import (\n",
        "    AIMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from PIL import Image\n",
        "from pydantic import BaseModel, Field, root_validator, validator\n",
        "from sklearn.preprocessing import normalize\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Another patch to make Colab work\n",
        "sys.stdout.fileno = lambda: 0\n",
        "sys.stderr.fileno = lambda: 1\n",
        "# End patch\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JWqfxzEiSRF_"
      },
      "source": [
        "First, load the Encord Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPUbwF0tSRaB"
      },
      "outputs": [],
      "source": [
        "project = Project(Path(project_name)).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g-00M506FBu"
      },
      "outputs": [],
      "source": [
        "class DatasetImage(BaseModel):\n",
        "    image: Path\n",
        "    data_hash: str\n",
        "\n",
        "\n",
        "iterator = DatasetIterator(project.file_structure.project_dir)\n",
        "\n",
        "# ğŸ—’ï¸ List all images in the project\n",
        "project_images: list[DatasetImage] = [\n",
        "    DatasetImage(\n",
        "        image=data_unit[1],\n",
        "        data_hash=iterator.du_hash,\n",
        "    )\n",
        "    for data_unit in iterator.iterate()\n",
        "]\n",
        "project_img_df = pd.DataFrame(project_images)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NwYzxSLpSe-E"
      },
      "source": [
        "You've loaded the image paths and associated data hashes to be able to match them to other queries later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt6n7JV1Sx8x"
      },
      "outputs": [],
      "source": [
        "project_img_df = pd.DataFrame([i.dict() for i in project_images])\n",
        "project_img_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KN89jj3CUW7Y"
      },
      "source": [
        "# ğŸ“Embedding Images with CLIP"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-z0b9z0p8hbK"
      },
      "source": [
        "In the following cells, you will learn how to embed images with CLIP. You will load in a bunch of images from the COCO Validation project and compute the CLIP embeddings.\n",
        "\n",
        "Next, you will see how to search these embeddings based on both new Images and on Text.\n",
        "\n",
        "Encord have made OpenAI's [CLIP model](https://github.com/openai/CLIP) available via PIP for ease of use.\n",
        "The dependency is already installed with `encord-active` so nothing needs to be done.\n",
        "\n",
        "However, if you want the dependency in isolation, you can install it with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp7Q5JqK9HVm"
      },
      "outputs": [],
      "source": [
        "#!python -m pip install clip-ea"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D7uEVi649vJf"
      },
      "source": [
        "With the installation, it's easy to instantiate a pretrained model to use for embedding images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVnGiMfK9HRf"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "print(f\"Model loaded on the {'CPU' if device == 'cpu' else 'GPU'}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WoZafMd1-QXx"
      },
      "source": [
        "Now embed some images. For starters, grab 1000 images and embed them in batches of 100 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U3uiQf29Gw7"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 100\n",
        "DB_SIZE = 1000\n",
        "image_list = project_img_df.image.to_list()\n",
        "db_images, unindexed_images = image_list[:DB_SIZE], image_list[DB_SIZE:]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def embed_images(model, images: list[Path], device):\n",
        "    out: list[np.ndarray] = []\n",
        "    for batch_start in tqdm(range(0, len(images), BATCH_SIZE)):\n",
        "        batch = images[batch_start : batch_start + BATCH_SIZE]\n",
        "        if not batch:\n",
        "            continue\n",
        "\n",
        "        batch_images = [preprocess(Image.open(i).convert(\"RGB\")) for i in batch]\n",
        "        if len(batch_images) == 1:\n",
        "            tensors = batch_images[0].to(device)[None]\n",
        "        else:\n",
        "            tensors = torch.stack(batch_images).to(device)\n",
        "        out.append(clip_model.encode_image(tensors).detach().cpu().numpy())\n",
        "\n",
        "    # create one np array with all images\n",
        "    if len(out) == 1:\n",
        "        return out[0]\n",
        "    return np.concatenate(out, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M9RU9dYBJNc"
      },
      "outputs": [],
      "source": [
        "t0 = perf_counter()\n",
        "embeddings = embed_images(clip_model, db_images, device=device)\n",
        "t1 = perf_counter()\n",
        "print(f\"Embedding {embeddings.shape[0]} images took {t1 - t0:.3f} seconds ({embeddings.shape[0] / (t1-t0):.3f} img/sec)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9lnbzGMuA60"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DhRiGpt4VSnZ"
      },
      "source": [
        "# ğŸ“Š See how it looks with Umap"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LLW8v35JhFcB"
      },
      "source": [
        "Umap is one of multiple ways of embedding high dimensional data into 2D, so we can plot it.\n",
        "Similar high-dimensional vectors should end up close to each other in the low-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n48aatIGFSXJ"
      },
      "outputs": [],
      "source": [
        "reducer = umap.UMAP(random_state=0)\n",
        "embeddings_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(*embeddings_2d.T)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S_BSqpWCVXOm"
      },
      "source": [
        "## âœ‚ï¸ Indexing and searching CLIP Embeddings"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9sUo7WwcC8MC"
      },
      "source": [
        "To be able to search embeddings efficiently, it makes sense to build an index over the embeddings for efficient searching.\n",
        "\n",
        "In this example, you'll keep it simple and build the index using `faiss`, as it's already available on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DKMrTCvC4ym"
      },
      "outputs": [],
      "source": [
        "index = IndexFlatIP(embeddings.shape[1])\n",
        "index.add(normalize(embeddings))\n",
        "# â˜ï¸ That's it really. Normalizing the vectors to unit norm makes the search equivalent to cosine similarity."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3uRqVDRZDwz9"
      },
      "source": [
        "With the index, you can now query the embeddings ğŸ”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR2UACA-FYoE"
      },
      "outputs": [],
      "source": [
        "random.seed(0)\n",
        "\n",
        "num_neighbors = 3\n",
        "num_tries = 5\n",
        "\n",
        "# Sample random image outside the ones in the index\n",
        "query_indices = random.sample(list(range(len(unindexed_images))), k=num_tries)\n",
        "query_images = [unindexed_images[i] for i in query_indices]\n",
        "\n",
        "# Do search and embedding\n",
        "query_embeddings = embed_images(clip_model, query_images, device=device)\n",
        "similarities, indices = index.search(normalize(query_embeddings), k=num_neighbors)\n",
        "query_2d = reducer.transform(query_embeddings)\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(num_tries, num_neighbors+2, figsize=(15, 15))\n",
        "for try_, (img, emb_2d, nn_similarities, nn_indices) in enumerate(zip(query_images, query_2d, similarities, indices)):\n",
        "    # Plot 2D embeddings\n",
        "    axs[try_, 0].scatter(*embeddings_2d.T)\n",
        "    axs[try_, 0].axis(\"off\")\n",
        "    axs[try_, 0].scatter(*emb_2d.T, c=\"red\")\n",
        "    axs[try_, 0].scatter(*embeddings_2d[nn_indices].T, c=\"orange\")\n",
        "\n",
        "    # Plot images\n",
        "    axs[try_, 1].set_title(\"Query Image\")\n",
        "    axs[try_, 1].imshow(Image.open(img))\n",
        "    axs[try_, 1].axis(\"off\")\n",
        "    for sim, neighbor, ax in zip(nn_similarities, nn_indices, axs[try_, 2:]):\n",
        "        ax.set_title(f\"Similarity: {sim:.3f}\")\n",
        "        ax.imshow(Image.open(db_images[neighbor]))\n",
        "        ax.axis(\"off\")\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4FEQMgDDRX6M"
      },
      "source": [
        "\n",
        "âœ¨ It gets even more powerful when you search via text embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1VUJUNLD5iF"
      },
      "outputs": [],
      "source": [
        "text_queries = [\n",
        "    \"surfing\",\n",
        "    \"motorbikes\",\n",
        "    \"transportation\",\n",
        "    \"red flowers in a vase\"\n",
        "]\n",
        "num_neighbors = 3\n",
        "num_tries = len(text_queries)\n",
        "\n",
        "# Do search and embeddings\n",
        "text_tensors = torch.concatenate([clip.tokenize(t) for t in text_queries], dim=0).to(device)\n",
        "query_embeddings = clip_model.encode_text(\n",
        "    text_tensors\n",
        ").detach().cpu().numpy()\n",
        "similarities, indices = index.search(normalize(query_embeddings), k=num_neighbors)\n",
        "query_2d = reducer.transform(query_embeddings)\n",
        "\n",
        "# Plot\n",
        "fig, axs = plt.subplots(num_tries, num_neighbors+1, figsize=(15, 12))\n",
        "for try_, (query, emb_2d, nn_similarities, nn_indices) in enumerate(zip(text_queries, query_2d, similarities, indices)):\n",
        "    # Plot 2D embeddings\n",
        "    axs[try_, 0].scatter(*embeddings_2d.T)\n",
        "    axs[try_, 0].axis(\"off\")\n",
        "    axs[try_, 0].scatter(*emb_2d.T, c=\"red\")\n",
        "    axs[try_, 0].scatter(*embeddings_2d[nn_indices].T, c=\"orange\")\n",
        "    axs[try_, 0].set_title(f'Query: \"{query}\"')\n",
        "\n",
        "    # Plot images\n",
        "    for sim, neighbor, ax in zip(nn_similarities, nn_indices, axs[try_, 1:]):\n",
        "        ax.set_title(f\"Similarity: {sim:.3f}\")\n",
        "        ax.imshow(Image.open(db_images[neighbor]))\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TvbJbXjkVz_V"
      },
      "source": [
        "#ğŸ” Indirect Search with ChatGPT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BqFWTikCSe5w"
      },
      "source": [
        "\n",
        "\n",
        "For this you'll use `langchain` to get started. So let's do that.\n",
        "\n",
        "Steps:\n",
        "1. Load Quality Metrics from the Encord Project\n",
        "2. Setup prompt\n",
        "3. Ask ChatGPT for help"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I7pSyo6vXBmb"
      },
      "source": [
        "Get the complete data frame from the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWWECTADW8dx"
      },
      "outputs": [],
      "source": [
        "ensure_initialised_merged_metrics(project.file_structure)\n",
        "with DBConnection(project.file_structure) as conn:\n",
        "    df = MergedMetrics(conn).all()\n",
        "\n",
        "df[\"data_hash\"] = df.index.str.split(\"_\", expand=False).str[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PqWlT6wlYtOX"
      },
      "source": [
        "A few insights from the table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNYmJaWIYtop"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.precision\", 3)\n",
        "print(df.describe().to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7oPPTgHMTCy_"
      },
      "outputs": [],
      "source": [
        "#@title ğŸ—ï¸ Set api key and instantiate model\n",
        "OPENAI_API_KEY = getpass(\"What's your OpenAI API key? \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "model_name = 'text-davinci-003'\n",
        "temperature = 0.0\n",
        "model = OpenAI(model_name=model_name, temperature=temperature)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y--DAmiZzZi"
      },
      "source": [
        "Prepare the prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQM3IVyaTBZl"
      },
      "outputs": [],
      "source": [
        "# Define the prompt that we'll be giving ChatGPT\n",
        "def form_prompt(dataframe, parser):\n",
        "    system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
        "        \"You are a helpful assistant that translates human queries to filters that apply to a data frame.\"\n",
        "    )\n",
        "    columns_str = \"\\n\".join(dataframe.columns)\n",
        "    instructions_prompt = HumanMessagePromptTemplate.from_template(\n",
        "        f\"Columns in the dataframe are: \\n{columns_str}\\n\\n\"\n",
        "        f\"Data frame description: \\n{dataframe.describe()}\\n\\n\"\n",
        "        \"Here are some rules:\\n\"\n",
        "        \"1. Top, highest, or largest means the highest quartile.\\n\"\n",
        "        \"2. Bottom, least, and lowest means the lowest quartile.\\n\"\n",
        "        \"3. `min_value` and `max_value` should be floats or ints related to the data frame description above.\\n\"\n",
        "        \"4. `min_value` cannot be larger than the `max_value`\\n\"\n",
        "        'If you are not able to answer, please respond with [{{filters: [{{\"column\": \"unknown\", \"min_value\": -1, \"max_value\": -1}}]}}\\n\\n'\n",
        "    )\n",
        "    query_prompt = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "            input_variables=[\"query\"],\n",
        "            partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        "        )\n",
        "    )\n",
        "    return ChatPromptTemplate.from_messages([system_message_prompt, instructions_prompt, query_prompt])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3pQHPOUS1am"
      },
      "outputs": [],
      "source": [
        "# Define pydantic model for filter outputs\n",
        "class Filter(BaseModel):\n",
        "    column: str = Field(description=\"The column of the provided dataframe to filter\")\n",
        "    min_value: float = Field(description=\"The minimum value to include\")\n",
        "    max_value: float = Field(description=\"The maximum value to include\")\n",
        "\n",
        "    @validator(\"column\")\n",
        "    def column_exists(cls, field):\n",
        "        if field == \"unknown\":\n",
        "            return field\n",
        "\n",
        "        if field not in df.columns:\n",
        "            raise ValueError(\"The specified column does not exist in the provided dataframe\")\n",
        "        return field\n",
        "\n",
        "    @root_validator()\n",
        "    def check_min_smaller_than_max(cls, values):\n",
        "        min_value = values.get(\"min_value\")\n",
        "        max_value = values.get(\"max_value\")\n",
        "\n",
        "        if not isinstance(min_value, (float, int)):\n",
        "            raise ValueError(f\"`min_value` should be a number\")\n",
        "\n",
        "        if not isinstance(max_value, (float, int)):\n",
        "            raise ValueError(f\"`max_value` should be a number\")\n",
        "\n",
        "        if min_value > max_value:\n",
        "            raise ValueError(f\"`min_value` ({min_value}) cannot be larger than `max_value` ({max_value})\")\n",
        "        return values\n",
        "\n",
        "class Filters(BaseModel):\n",
        "    filters: list[Filter] = Field(description=\"A list of filters needed to be applied in given order\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2tWeDwlxTni4"
      },
      "source": [
        "See an example of what you would pass to ChatGPT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDcqbJoQTt-4"
      },
      "outputs": [],
      "source": [
        "parser = PydanticOutputParser(pydantic_object=Filters)\n",
        "example_query = \"What are all the images with both high contrast and many objects?\"\n",
        "input_prompt = form_prompt(df, parser).format_prompt(query=example_query, format_instructions=parser.get_format_instructions())\n",
        "pprint(input_prompt.to_string())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IsaaXt3KTxph"
      },
      "source": [
        "And now the final bit, which is stitching it all together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAeOhb_FZyjh"
      },
      "outputs": [],
      "source": [
        "def do_indirect_query(model, query:str, dataframe: pd.DataFrame):\n",
        "    # Generate the prompt\n",
        "    parser = PydanticOutputParser(pydantic_object=Filters)\n",
        "    input_prompt = form_prompt(dataframe, parser).format_prompt(query=query, format_instructions=parser.get_format_instructions())\n",
        "\n",
        "    # Ask ChatGPT for help given the prompt\n",
        "    response = model(input_prompt.to_string())\n",
        "\n",
        "    # Parse the output with a retry\n",
        "    output: Filters | None = None\n",
        "    try:\n",
        "         output = parser.parse(response)\n",
        "    except:\n",
        "        print(f\"Trying to fix error after receiving {response}\")\n",
        "        retry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=OpenAI(temperature=0))\n",
        "        try:\n",
        "            output = retry_parser.parse_with_prompt(response, input_prompt)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if not output or not output.filters or output.filters[0].column == \"unknown\":\n",
        "        print(f\"This query couldn't be processed properly. The response gotten from ChatGPT was: {response}\")\n",
        "        return None\n",
        "\n",
        "    # Do the actual filtering\n",
        "    subset_df = df.copy()\n",
        "    for filter in output.filters:\n",
        "        subset_df = subset_df[subset_df[filter.column].between(filter.min_value, filter.max_value, inclusive=\"both\")]\n",
        "    return subset_df.sort_values([f.column for f in output.filters], ascending=False).reset_index(), output\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kR9GYoVQUvnJ"
      },
      "source": [
        "Try it out for an example query on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36q2RwIKUv7O"
      },
      "outputs": [],
      "source": [
        "example_query = \"What are all the images with both high contrast and many objects?\"\n",
        "subset_df, filters = do_indirect_query(model, example_query, df)\n",
        "print(f\"Number of results: {subset_df.shape[0]}\")\n",
        "print(\"Filters\")\n",
        "pprint(filters)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V060nwWHVBli"
      },
      "source": [
        "Plot the results to see the actual images found based on the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBFiNBSinb-s"
      },
      "outputs": [],
      "source": [
        "def plot_top_k_data_units(df, filters: Filters, k=12, cols=3):\n",
        "    rows = k // cols if k % cols == 0 else k // cols + 1\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
        "    axs = axs.reshape(-1)\n",
        "    fig.suptitle(\"; \".join(map(lambda f: f.column, filters.filters)))\n",
        "\n",
        "    for (idx, row), ax in zip(df.iterrows(), axs):\n",
        "        img = show_image_and_draw_polygons(row, project.file_structure)\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(\"; \".join([f\"{row[f.column]:.3f}\" for f in filters.filters]))\n",
        "        ax.axis(\"off\")\n",
        "    fig.tight_layout()\n",
        "    return fig\n",
        "\n",
        "_ = plot_top_k_data_units(subset_df, filters)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q3ZB2omWVZO"
      },
      "source": [
        "# ğŸª¢ Putting it all together"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-zc9_Mwaj2xh"
      },
      "source": [
        "\n",
        "\n",
        "Now that you know how to do direct semantic queries with CLIP and indirect semantic queries with ChatGPT, combine them.\n",
        "\n",
        "The steps are:\n",
        "\n",
        "1. Compute embeddings for the entire dataset\n",
        "2. Define some direct and indirect query pairs\n",
        "3. Use an index to find the nearest neighbors based on CLIP Embeddings\n",
        "4. Use ChatGPT to refine the search by indirect queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9M2B991WqPU"
      },
      "outputs": [],
      "source": [
        "# Set some thresholds for the CLIP search\n",
        "num_neighbors = 1000\n",
        "similarity_threshold = 0.265  # ğŸ‘ˆ The minimum similarity required to be considered relevant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0YSFqh81utW"
      },
      "outputs": [],
      "source": [
        "# Embed the entire dataset\n",
        "t0 = perf_counter()\n",
        "project_embeddings = embed_images(clip_model, image_list, device=device)\n",
        "t1 = perf_counter()\n",
        "print(f\"Embedding {project_embeddings.shape[0]} images took {t1 - t0:.3f} seconds ({project_embeddings.shape[0] / (t1-t0):.3f} img/sec)\")\n",
        "\n",
        "# Create an index\n",
        "project_index = IndexFlatIP(project_embeddings.shape[1])\n",
        "project_index.add(normalize(project_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OFvWXNcWAZT"
      },
      "outputs": [],
      "source": [
        "# Define queries\n",
        "direct_queries = [\n",
        "    \"outdoor sports\",\n",
        "    \"transportation\"\n",
        "]\n",
        "indirect_queries = [\n",
        "    \"All the images with high brightness and many objects\",\n",
        "    \"All the objects with high annotation quality\"\n",
        "]\n",
        "\n",
        "# Embed direct queries\n",
        "text_tensors = torch.concatenate([clip.tokenize(t) for t in direct_queries], dim=0).to(device)\n",
        "query_embeddings = clip_model.encode_text(\n",
        "    text_tensors\n",
        ").detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE-bf50CWFMG"
      },
      "outputs": [],
      "source": [
        "# Do the direct semantic querying\n",
        "similarities, indices = index.search(normalize(query_embeddings), k=num_neighbors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQRO6M_2j2C1"
      },
      "outputs": [],
      "source": [
        "# Filter dataframe based on search result.\n",
        "for (in_query, di_query, sim, idx) in zip(indirect_queries, direct_queries, similarities, indices):\n",
        "    idx = idx[sim>similarity_threshold]\n",
        "    data_hashes = set(project_img_df.iloc[idx].data_hash.to_list())\n",
        "\n",
        "    filtered_df = df.copy()\n",
        "\n",
        "    # Filter project data\n",
        "    clip_filtered_df = filtered_df[filtered_df.data_hash.isin(data_hashes)]\n",
        "    gpt_result = do_indirect_query(model, in_query, clip_filtered_df)\n",
        "\n",
        "    if gpt_result is None:\n",
        "        print(f\"Chat GPT failed to produce valid filters for the indirect query {in_query}\")\n",
        "        continue\n",
        "\n",
        "    gpt_filtered_df, filters = gpt_result\n",
        "\n",
        "    print(f\"Results for direct query: '{di_query}' and indirect query: '{in_query}'\")\n",
        "    print(f\"Found {gpt_filtered_df.shape[0]} results matching the query based of {clip_filtered_df.shape[0]} semantically similar images.\")\n",
        "    print(f\"Based on filters: {filters}\")\n",
        "    print(\"- \" * 10)\n",
        "    fig = plot_top_k_data_units(gpt_filtered_df, filters)\n",
        "    fig.suptitle(f\"IQ: '{in_query}', DQ: '{di_query}'\", fontsize=16)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MQITOFpyWg9n"
      },
      "source": [
        "# âœ… Wrap up"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4CPsdC2YBfj"
      },
      "source": [
        "\n",
        "ğŸ““This Colab notebook showed you how to build a semantic search engine for visual search data using CLIP and ChatGPT.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸŸ£ Encord Active is an open-source framework for computer vision model testing, evaluation, and validation. Check out the project on [GitHub](https://github.com/encord-team/encord-active), leave a star ğŸŒŸ if you like it, and leave an issue if you find something is missing.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ‘‰ Check out our ğŸ“–[blog](https://encord.com/blog/webinar-semantic-visual-search-chatgpt-clip/) and ğŸ“º[YouTube](https://www.youtube.com/@encord) channel to stay up-to-date with the latest in computer vision, foundation models, active learning, and data-centric AI.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IOF2pqteYRF4"
      },
      "source": [
        "### âœ¨ Want more walthroughs like this? Check out the ğŸŸ£ [Encord Notebooks repository](https://github.com/encord-team/encord-notebooks/)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
